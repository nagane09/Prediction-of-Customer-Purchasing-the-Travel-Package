# Prediction of Customer Purchasing the Travel Package

ğŸ”— **Live Demo:** https://prediction-of-customer-purchasing-the-rsab.onrender.com/

Predict whether a customer will purchase a travel package based on historical customer data and purchase trends using machine learning models.

---
# ğŸ–ï¸ Holiday Package Purchase Prediction

## ğŸ“Œ Project Overview

This project predicts whether a customer will **purchase a holiday travel package** based on demographic, behavioral, and interaction-based features.

The solution helps travel companies:
- Identify **high-potential customers**
- Optimize **targeted marketing**
- Reduce **customer acquisition cost**
- Increase **conversion rate**

This is a **binary classification problem**:
- `1` â†’ Customer purchased the package  
- `0` â†’ Customer did not purchase the package  

---

## ğŸ“Š Dataset Information

- **Source**: Kaggle  
- **Dataset Name**: Holiday Package Purchase Prediction  
- **Total Records**: ~4,800  
- **Target Variable**: `ProdTaken`

### ğŸ“ Feature Description

| Feature | Description |
|------|------------|
| Age | Customer age |
| TypeofContact | How customer was contacted |
| CityTier | City classification (1â€“3) |
| DurationOfPitch | Sales pitch duration |
| Occupation | Customer occupation |
| Gender | Customer gender |
| ProductPitched | Package offered |
| PreferredPropertyStar | Hotel rating preference |
| MaritalStatus | Marital status |
| NumberOfTrips | Previous trips |
| Passport | Passport availability |
| PitchSatisfactionScore | Pitch satisfaction |
| OwnCar | Car ownership |
| MonthlyIncome | Monthly income |
| TotalVisiting | Total people traveling |

---

## ğŸ§¹ Data Cleaning & Preprocessing

### âœ”ï¸ Missing Value Treatment

| Feature | Strategy |
|------|---------|
| Age | Median |
| DurationOfPitch | Median |
| MonthlyIncome | Median |
| TypeofContact | Mode |
| NumberOfTrips | Median |
| PreferredPropertyStar | Mode |
| NumberOfChildrenVisiting | Mode |

### âœ”ï¸ Data Corrections
- `Fe Male` â†’ `Female`
- `Single` â†’ `Unmarried`
- Dropped `CustomerID` (identifier)

---

## ğŸ§  Feature Engineering

### â• New Feature
```text
TotalVisiting = NumberOfPersonVisiting + NumberOfChildrenVisiting
```

## ğŸ”„ Data Transformation

- **Categorical Features** â†’ One-Hot Encoding  
- **Numerical Features** â†’ Standard Scaling  
- Implemented using **ColumnTransformer** to ensure a clean and reusable ML pipeline.

---

## ğŸ”€ Train-Test Split

- **Training Set**: 80%  
- **Testing Set**: 20%  
- **Random State**: 42  

---

## ğŸ¤– Model Selection

Multiple classification models were initially experimented with.  
Based on performance, generalization ability, and business relevance, **XGBoost** was selected as the **final model**.

---

## ğŸ† Final Model: XGBoost Classifier

XGBoost delivered the **best balance between accuracy, recall, and ROC-AUC**, making it suitable for identifying customers most likely to purchase a holiday package.

### ğŸ”¥ Performance Before Hyperparameter Tuning

| Metric | Train | Test |
|------|------|------|
| Accuracy | 99.92% | 93.56% |
| F1 Score | 99.92% | 93.18% |
| Recall | 99.59% | 70.68% |
| ROC-AUC | 0.99 | 0.84 |

---

## âš™ï¸ Hyperparameter Tuning

Hyperparameter tuning was performed to reduce overfitting and improve generalization.

### Best XGBoost Parameters
```json
{
  "n_estimators": 200,
  "max_depth": 8,
  "learning_rate": 0.1,
  "colsample_bytree": 0.8
}
```
## ğŸ¥‡ Final Model Performance (Tuned XGBoost)

| Metric   | Train | Test |
|---------|-------|------|
| Accuracy | 100%  | 94.99% |
| F1 Score | 100%  | 94.75% |
| Recall   | 100%  | ~75% |
| ROC-AUC  | 1.00  | ~0.85 |

âœ… The tuned **XGBoost** model demonstrates **strong predictive performance** while maintaining **good generalization** on unseen data.

---

### ğŸ“Š Why This Works Well for This Problem

- Captures **non-linear relationships** between customer attributes  
- Handles **feature interactions** automatically  
- Regularization controls overfitting despite high training accuracy  
- Gradient-based optimization improves **recall for minority class**

---

### âœ… Summary

XGBoost combines:
- Gradient descent optimization  
- Tree-based feature learning  
- Explicit regularization  

This results in a **highly accurate, well-generalized model** for predicting holiday package purchases.


---
